{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["\n","from metrics import plot_confusion\n","from IPython.display import Image\n","from keras.utils import plot_model\n","import numpy as np\n","import tensorflow.keras.backend as k\n","from tensorflow.keras.layers import Dense, Input, Layer, InputSpec\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.initializers import VarianceScaling\n","from sklearn.cluster import KMeans\n","import metrics\n","from data import load_data\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading ./data/conll_spacy_10000.pkl\n","Done: (37748, 772)\n","Train data balance:\n","[[   2 1454]\n"," [   3  102]\n"," [   4 1344]\n"," [   5 1842]\n"," [   6   80]\n"," [   7   21]\n"," [   8   32]\n"," [   9    8]\n"," [  10    5]\n"," [  11    5]\n"," [  12 1100]\n"," [  13  101]\n"," [  14   96]\n"," [  15   73]\n"," [  16   75]\n"," [  17    6]\n"," [  18   44]\n"," [  19   69]]\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Will\\Anaconda3\\envs\\tf-27\\lib\\site-packages\\sklearn\\utils\\validation.py:1858: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["x: (33156, 768), y: (33156,)\n","{0: 'PERSON', 1: 'NORP', 2: 'ORG', 3: 'GPE', 4: 'LOC', 5: 'PRODUCT', 6: 'EVENT', 7: 'WORK_OF_ART', 8: 'LANGUAGE', 9: 'LAW', 10: 'DATE', 11: 'TIME', 12: 'PERCENT', 13: 'MONEY', 14: 'QUANTITY', 15: 'ORDINAL', 16: 'CARDINAL', 17: 'FAC'}\n","(33156, 768)\n"]}],"source":["size = 10000\n","max_iter = 140\n","# entities = ['ORG', 'GPE', 'PERSON', ]  # , 'NORP',  ]\n","x, y, mapping, strings = load_data(size, get_text=True)\n","n_clusters = len(np.unique(y))+10\n","print(x.shape)\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["\n","def autoencoder_model(dims, act='relu', init='glorot_uniform'):\n","    \"\"\"\n","    Fully connected auto-encoder model, symmetric.\n","    Arguments:\n","        dims: list of number of units in each layer of encoder. dims[0] is\n","            input dim, dims[-1] is units in hidden layer.\n","            The decoder is symmetric with encoder. So number of layers of\n","            the auto-encoder is 2*len(dims)-1\n","        act: activation, not applied to Input, Hidden and Output layers\n","    return:\n","        (ae_model, encoder_model), Model of autoencoder and model of encoder\n","    \"\"\"\n","    n_stacks = len(dims) - 1\n","    # input\n","    input_img = Input(shape=(dims[0],), name='input')\n","    x = input_img\n","    # internal layers in encoder\n","    for i in range(n_stacks-1):\n","        x = Dense(dims[i + 1], activation=act,\n","                  kernel_initializer=init, name=f'encoder_{i}')(x)\n","\n","    # hidden layer\n","    # hidden layer, features are extracted from here\n","    encoded = Dense(dims[-1], kernel_initializer=init,\n","                    name=f'encoder_{n_stacks - 1}')(x)\n","\n","    x = encoded\n","    # internal layers in decoder\n","    for i in range(n_stacks-1, 0, -1):\n","        x = Dense(dims[i], activation=act,\n","                  kernel_initializer=init, name=f'decoder_{i}')(x)\n","\n","    # output\n","    x = Dense(dims[0], kernel_initializer=init, name='decoder_0')(x)\n","    decoded = x\n","    return (Model(inputs=input_img, outputs=decoded, name='AE'),\n","            Model(inputs=input_img, outputs=encoded, name='encoder'))\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["kmeans = KMeans(n_clusters=n_clusters, n_init=20)\n","y_pred_kmeans = kmeans.fit_predict(x)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["metrics.acc(y, y_pred_kmeans)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dims = [x.shape[-1], 500, 500, 2000, 50]\n","init = VarianceScaling(scale=1. / 3., mode='fan_in',\n","                       distribution='uniform')\n","pretrain_optimizer = SGD(lr=1, momentum=0.9)\n","pretrain_epochs = 300\n","batch_size = 256\n","save_dir = './results'\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","class ClusteringLayer(Layer):\n","    \"\"\"\n","    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n","    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n","\n","    # Example\n","    ```\n","        model.add(ClusteringLayer(n_clusters=10))\n","    ```\n","    # Arguments\n","        n_clusters: number of clusters.\n","        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n","        alpha: degrees of freedom parameter in Student's t-distribution. Default to 1.0.\n","    # Input shape\n","        2D tensor with shape: `(n_samples, n_features)`.\n","    # Output shape\n","        2D tensor with shape: `(n_samples, n_clusters)`.\n","    \"\"\"\n","\n","    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n","        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n","            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n","        super(ClusteringLayer, self).__init__(**kwargs)\n","        self.n_clusters = n_clusters\n","        self.alpha = alpha\n","        self.initial_weights = weights\n","        self.input_spec = InputSpec(ndim=2)\n","\n","    def build(self, input_shape):\n","        assert len(input_shape) == 2\n","        input_dim = input_shape[1]\n","        self.input_spec = InputSpec(dtype=k.floatx(), shape=(None, input_dim))\n","        self.clusters = self.add_weight(\n","            shape=(self.n_clusters, input_dim),\n","            initializer='glorot_uniform',\n","            name='clusters')\n","        if self.initial_weights is not None:\n","            self.set_weights(self.initial_weights)\n","            del self.initial_weights\n","        self.built = True\n","\n","    def call(self, inputs, **kwargs):\n","        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n","         Measure the similarity between embedded point z_i and centroid µ_j.\n","                 q_ij = 1/(1+dist(x_i, µ_j)^2), then normalize it.\n","                 q_ij can be interpreted as the probability of assigning sample i to cluster j.\n","                 (i.e., a soft assignment)\n","        Arguments:\n","            inputs: the variable containing data, shape=(n_samples, n_features)\n","        Return:\n","            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n","        \"\"\"\n","        q = 1.0 / (1.0 + (k.sum(k.square(k.expand_dims(inputs,\n","                   axis=1) - self.clusters), axis=2) / self.alpha))\n","        q **= (self.alpha + 1.0) / 2.0\n","        # Make sure each sample's 10 values add up to 1.\n","        q = k.transpose(k.transpose(q) / k.sum(q, axis=1))\n","        return q\n","\n","    def compute_output_shape(self, input_shape):\n","        assert input_shape and len(input_shape) == 2\n","        return input_shape[0], self.n_clusters\n","\n","    def get_config(self):\n","        config = {'n_clusters': self.n_clusters}\n","        base_config = super(ClusteringLayer, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["autoencoder, encoder = autoencoder_model(dims, init=init)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["autoencoder.compile(optimizer=pretrain_optimizer, loss='mse')\n","autoencoder.fit(x, x, batch_size=batch_size,\n","                epochs=pretrain_epochs)  # , callbacks=cb)\n","autoencoder.save_weights(save_dir + '/jae_weights.h5')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["autoencoder.load_weights(save_dir+'/jae_weights.h5')\n","clustering_layer = ClusteringLayer(\n","    n_clusters, name='clustering')(encoder.output)\n","model = Model(inputs=encoder.input,\n","              outputs=[clustering_layer, autoencoder.output])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_model(model, to_file='model.png', show_shapes=True)\n","Image(filename='model.png')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["kmeans = KMeans(n_clusters=n_clusters, n_init=20)\n","y_pred = kmeans.fit_predict(encoder.predict(x))\n","model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n","y_pred_last = np.copy(y_pred)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.compile(loss=['kld', 'mse'], loss_weights=[\n","              0.1, 1], optimizer=pretrain_optimizer)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["loss = 0\n","index = 0\n","maxiter = 8000\n","update_interval = 140\n","index_array = np.arange(x.shape[0])\n","tol = 0.001  # tolerance threshold to stop training\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","def target_distribution(q):\n","    weight = q ** 2 / q.sum(0)\n","    return (weight.T / weight.sum(1)).T\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for ite in range(int(maxiter)):\n","    if ite % update_interval == 0:\n","        q, _ = model.predict(x, verbose=0)\n","        # update the auxiliary target distribution p\n","        p = target_distribution(q)\n","\n","        # evaluate the clustering performance\n","        y_pred = q.argmax(1)\n","        if y is not None:\n","            acc = np.round(metrics.acc(y, y_pred), 5)\n","            nmi = np.round(metrics.nmi(y, y_pred), 5)\n","            ari = np.round(metrics.ari(y, y_pred), 5)\n","            loss = np.round(loss, 5)\n","            print('Iter %d: acc = %.5f, nmi = %.5f, ari = %.5f' %\n","                  (ite, acc, nmi, ari), ' ; loss=', loss)\n","\n","        # check stop criterion\n","        delta_label = np.sum(y_pred != y_pred_last).astype(\n","            np.float32) / y_pred.shape[0]\n","        y_pred_last = np.copy(y_pred)\n","        if ite > 0 and delta_label < tol:\n","            print('delta_label ', delta_label, '< tol ', tol)\n","            print('Reached tolerance threshold. Stopping training.')\n","            break\n","    idx = index_array[index *\n","                      batch_size: min((index+1) * batch_size, x.shape[0])]\n","    loss = model.train_on_batch(x=x[idx], y=[p[idx], x[idx]])\n","    index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n","\n","model.save_weights(save_dir + '/b_DEC_model_final.h5')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","model.load_weights(save_dir + '/b_DEC_model_final.h5')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Eval.\n","q, _ = model.predict(x, verbose=0)\n","p = target_distribution(q)  # update the auxiliary target distribution p\n","\n","# evaluate the clustering performance\n","y_pred = q.argmax(1)\n","if y is not None:\n","    acc = np.round(metrics.acc(y, y_pred), 5)\n","    nmi = np.round(metrics.nmi(y, y_pred), 5)\n","    ari = np.round(metrics.ari(y, y_pred), 5)\n","    loss = np.round(loss, 5)\n","    print('Acc = %.5f, nmi = %.5f, ari = %.5f' %\n","          (acc, nmi, ari), ' ; loss=', loss)\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(np.unique(y_pred))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(mapping)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_confusion(y, y_pred, mapping, 8)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","\n","print (\"CLUSTERS\")\n","predicted = pd.DataFrame({'text':strings, 'y_pred':y_pred, 'y_true':y})\n","for cluster_no in range(n_clusters):\n","    y_pred_for_key = predicted[predicted['y_pred']==cluster_no]\n","    true_label = 'UNKNOWN'\n","    mode = y_pred_for_key['y_true'].mode()\n","    if len(mode)>0:\n","        true_label_n = y_pred_for_key['y_true'].mode()[0]\n","        if true_label_n in mapping:\n","            true_label = mapping[true_label_n]\n","    print(f\"Cluster {cluster_no}: {true_label}\")\n","    unique, counts = np.unique(y_pred_for_key['text'], return_counts=True)\n","\n","    freq_list = np.asarray((unique, counts)).T\n","    freqs = {w: f for w,f in freq_list}\n","    if len(freqs) > 0:\n","        wc = WordCloud().generate_from_frequencies(freqs)\n","        plt.figure(figsize=(16, 14))\n","        plt.imshow(wc, interpolation='bilinear')\n","        plt.axis(\"off\")\n","        plt.show()\n","    else:\n","        print(f\"No words for cluster {true_label}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.12 ('tf-27')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"18e4ecf883516c2f5e8920d96ffc8522d588da1ae71a3f5d592afbc91cfc38c7"}}},"nbformat":4,"nbformat_minor":2}
